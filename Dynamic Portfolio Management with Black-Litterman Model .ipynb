{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f688825-d256-4e1b-83c0-c9b0730fadc7",
   "metadata": {},
   "source": [
    "- Black-Litterman Model Integration:\n",
    "Use the forecasts from the ML models (e.g., LSTM predicted returns) as “views” in the Black-Litterman (BL) framework to derive a new implied return vector and construct a posterior distribution of returns. We will then solve for the BL-implied optimal portfolio weights.\n",
    "- Reinforcement Learning for Dynamic Rebalancing:\n",
    "Create a reinforcement learning (RL) environment that simulates a portfolio rebalancing scenario. The RL agent will learn to adjust portfolio weights over time to maximize a performance metric (like risk-adjusted returns). Transaction costs will be included in the reward function to encourage cost-effective trading.\n",
    "- Risk Parity Approach:\n",
    "Implement a simple risk parity portfolio solution as an alternative or baseline approach.\n",
    "- Transaction Costs:\n",
    "Incorporate transaction costs into both the BL optimization (as part of the optimization constraints or objective) and the RL environment’s reward structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52f806-4307-4f75-a5b9-f5ba03aca2c8",
   "metadata": {},
   "source": [
    "# Black-Litterman Equations Used\n",
    "\n",
    "## Given:\n",
    "\n",
    "* Prior expected returns vector: μ (equilibrium returns)\n",
    "* Covariance matrix: Σ\n",
    "* Views matrix: P, views vector: Q\n",
    "* Uncertainty parameter: τ\n",
    "* View uncertainty matrix: Ω\n",
    "\n",
    "## The Black-Litterman posterior is:\n",
    "\n",
    "μ<sub>post</sub> = μ + τΣP<sup>T</sup>(PτΣP<sup>T</sup> + Ω)<sup>-1</sup>(Q - Pμ)\n",
    "\n",
    "Σ<sub>post</sub> = Σ + ΣP<sup>T</sup>(PτΣP<sup>T</sup> + Ω)<sup>-1</sup>PτΣ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c648180-bccb-414a-86ac-e8d1ea3180ae",
   "metadata": {},
   "source": [
    "##  Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0fc3f-430f-4fc2-aaf4-392cc6ce0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fetch_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical price data for given tickers.\n",
    "    :param tickers: List of stock tickers.\n",
    "    :param start_date: Start date for the historical data.\n",
    "    :param end_date: End date for the historical data.\n",
    "    :return: DataFrame with adjusted closing prices.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "    return data\n",
    "\n",
    "tickers = [\"BWXT\", \"CEG\", \"NEE\", \"VRT\"]  # Add 'SWEC-B.ST' if supported\n",
    "data = yf.download(tickers, start=\"2022-02-01\", end=\"2024-12-05\")['Adj Close']\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cccf9-a865-4679-988d-78a52a7317f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute daily returns\n",
    "returns = data.pct_change().dropna()\n",
    "print(returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3d124-321b-4a9f-a6a2-747cb8bd500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_size = int(len(returns) * 0.83)\n",
    "train_returns = returns.iloc[:train_size]\n",
    "test_returns = returns.iloc[train_size:]\n",
    "\n",
    "print(\"Train set length:\", len(train_returns))\n",
    "print(\"Test set length:\", len(test_returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a168d03-d2cc-4eb9-8374-ae9c4d505dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta\n",
    "\n",
    "price_data = data.copy()  # Just renaming for clarity\n",
    "# Example: Compute RSI for each ticker\n",
    "for ticker in price_data.columns:\n",
    "    price_data[f\"{ticker}_RSI\"] = ta.momentum.RSIIndicator(price_data[ticker], window=14).rsi()\n",
    "\n",
    "# Drop rows with NaN caused by indicator computation\n",
    "price_data.dropna(inplace=True)\n",
    "\n",
    "print(price_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52b859-d5ef-4a5d-8e90-a6870d975871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 5-day ahead returns as target\n",
    "horizon = 5\n",
    "for ticker in data.columns:\n",
    "    price_data[f\"{ticker}_future_return\"] = data[ticker].pct_change(horizon).shift(-horizon)\n",
    "\n",
    "# Drop rows with NaN in the future returns\n",
    "price_data.dropna(inplace=True)\n",
    "\n",
    "print(price_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a93950-6b0f-467e-884d-2f762565c6ba",
   "metadata": {},
   "source": [
    "## Compute Prior (Market Equilibrium) Parameters\n",
    "\n",
    "We use historical mean returns and covariance from the training set as the “prior”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278661e-b19c-41ca-b2e6-d57a014eb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find rows with inf or very large values\n",
    "problematic_rows = train_returns[(np.isinf(train_returns)) | (train_returns > 1e5) | (train_returns < -1e5)]\n",
    "print(problematic_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4eabf-4641-405a-a0f7-738e2cd178fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"NaN values per column:\")\n",
    "print(train_returns.isna().sum())\n",
    "\n",
    "print(\"Number of Infinite values:\")\n",
    "print(np.isinf(train_returns).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f12215-28ac-4500-b4a3-873ded67d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows with infinite or extremely large values\n",
    "bad_rows = train_returns[(np.isinf(train_returns)) | (train_returns.abs() > 1e5)]\n",
    "print(\"Problematic rows:\\n\", bad_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205b4c5-989f-499a-9f9c-c701cb681c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_returns = train_returns.replace([np.inf, -np.inf], np.nan)\n",
    "train_returns = train_returns.dropna(how='any')  # Drop rows with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553398e6-ff46-4166-8a0f-fb6b6a82cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When computing returns:\n",
    "returns = price_data.pct_change().replace([np.inf, -np.inf], np.nan).dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd22e91-6a3f-4c8f-87f7-ad68fc107a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose 'VRT' is problematic:\n",
    "train_returns = train_returns.drop(columns=['VRT'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349b48d-f5d4-4010-969e-cc57fe8dea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove days where prices are zero or negative (if any):\n",
    "price_data = price_data[price_data > 0].dropna(how='any')\n",
    "returns = price_data.pct_change().dropna()\n",
    "# Then create train_returns again and retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c546bcd-a7e1-44d4-b998-65a03e1eddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "\n",
    "# Prior: Historical mean returns & covariance\n",
    "mu = mean_historical_return(train_returns)    # vector of prior expected returns\n",
    "S = CovarianceShrinkage(train_returns).ledoit_wolf()  # covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac882739-b8c1-4797-ae66-8f6529349b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = mean_historical_return(train_returns)\n",
    "S = CovarianceShrinkage(train_returns).ledoit_wolf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fccd0b-c86a-4b4b-963e-0e2a140710ec",
   "metadata": {},
   "source": [
    "## Construct Views from ML Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53155424-3fc3-43ee-81d9-cebcfc9dfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock ML predicted returns (views):\n",
    "view_returns = pd.Series([0.001, 0.0015, 0.0005], index=mu.index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3092be7-3efd-406c-b0c9-20be307d4214",
   "metadata": {},
   "source": [
    "## Form Views (P, Q, Omega):\n",
    "\t•\tWe have 3 views: one for each asset’s return. P is the identity matrix since we have direct views on each asset’s return level.\n",
    "\t•\tQ is the vector of view returns.\n",
    "\t•\tOmega represents the uncertainty in views. We choose it as a small diagonal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f0a9c-fb17-4f24-8d40-358d9acf3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = mu.index.tolist()\n",
    "n = len(assets)\n",
    "P = np.eye(n)\n",
    "Q = view_returns.values\n",
    "tau = 0.05\n",
    "\n",
    "# Omega: If we assume we trust our views moderately, set Omega proportional to diag of τΣ\n",
    "Omega = np.diag(np.diag(P @ (tau * S.values) @ P.T))  # same dimension as P @ S @ P^T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3beae30-b347-4278-94cb-05a3f4f3c750",
   "metadata": {},
   "source": [
    "## Black-Litterman Posterior Calculation (Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eeee55-4710-4e2d-afe0-b7342b047e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior mean and covariance\n",
    "Sigma = S.values\n",
    "mu_prior = mu.values.reshape(-1,1)\n",
    "\n",
    "A = np.linalg.inv(P @ (tau * Sigma) @ P.T + Omega)\n",
    "mu_post = mu_prior + tau * Sigma @ P.T @ A @ (Q - P @ mu_prior.flatten())\n",
    "\n",
    "Sigma_post = S.values + (tau * Sigma) - (tau * Sigma @ P.T @ A @ P @ (tau * Sigma))\n",
    "\n",
    "mu_post = pd.Series(mu_post.flatten(), index=mu.index)\n",
    "Sigma_post = pd.DataFrame(Sigma_post, index=mu.index, columns=mu.index)\n",
    "\n",
    "print(\"Posterior Mean Returns:\\n\", mu_post)\n",
    "print(\"Posterior Covariance:\\n\", Sigma_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ef3b1-3e62-489a-ac57-7840cabe8a36",
   "metadata": {},
   "source": [
    "## Optimize Portfolio with Transaction Costs (Static Optimization)\n",
    "\n",
    "Assume we have a current portfolio and want to find a new allocation that maximizes expected returns minus risk penalties and transaction costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72651510-8fc0-4c6c-90cd-efab905ebebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose current weights:\n",
    "w_current = np.array([0.4, 0.4, 0.2])\n",
    "\n",
    "w = cp.Variable(n)\n",
    "expected_return = mu_post.values @ w\n",
    "risk_aversion = 10\n",
    "lambda_tc = 0.001\n",
    "portfolio_var = cp.quad_form(w, Sigma_post.values)\n",
    "transaction_costs = lambda_tc * cp.norm(w - w_current, 1)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(expected_return - risk_aversion*portfolio_var - transaction_costs),\n",
    "                  [cp.sum(w) == 1,\n",
    "                   w >= 0])\n",
    "prob.solve()\n",
    "\n",
    "w_opt = w.value\n",
    "print(\"Optimized portfolio weights from Black-Litterman with TC:\", w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63904d2-e144-4e74-a89e-5b95c5c62be3",
   "metadata": {},
   "source": [
    "## Risk Parity Approach\n",
    "\n",
    "Implement a simple risk parity solution as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260df9c-82d4-4843-856b-bfab0d078d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_contributions(weights, cov):\n",
    "    w = weights.reshape(-1,1)\n",
    "    total_variance = (w.T @ cov @ w).item()\n",
    "    mc = cov @ w\n",
    "    rc = (w * mc) / total_variance\n",
    "    return rc.flatten()\n",
    "\n",
    "def risk_parity(cov, tol=1e-6, max_iter=1000):\n",
    "    n = cov.shape[0]\n",
    "    w = np.ones(n)/n\n",
    "    for _ in range(max_iter):\n",
    "        rc = risk_contributions(w, cov)\n",
    "        # We want all RC equal, target = 1/n\n",
    "        diff = rc - 1/n\n",
    "        if np.linalg.norm(diff) < tol:\n",
    "            break\n",
    "        # Gradient step:\n",
    "        w = w - 0.01 * diff\n",
    "        w = np.clip(w, 0, None)\n",
    "        w = w / w.sum()\n",
    "    return w\n",
    "\n",
    "rp_weights = risk_parity(Sigma_post.values)\n",
    "print(\"Risk parity weights:\", rp_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3180d7-c8dd-4d7b-9adf-a5acaab041bc",
   "metadata": {},
   "source": [
    "##  Reinforcement Learning for Dynamic Rebalancing\n",
    "\n",
    "We create a custom environment where the agent decides how to rebalance daily. The observation includes predicted returns and current weights, and the reward is the daily portfolio return minus transaction costs. The agent learns to rebalance over time.\n",
    "\n",
    "Note: This is a simplified environment. In practice, you’d incorporate more complex features (volatility forecasts, regimes, etc.), and train for many timesteps. Also ensure stable-baselines3 and gymnasium are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f9638-9209-4a01-9d6b-98d35fa614fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"gymnasium[classic_control]\"  # Example, depends on environment\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, price_data, predicted_returns, initial_capital=100000, transaction_cost=0.001):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        self.price_data = price_data\n",
    "        self.predicted_returns = predicted_returns\n",
    "        self.initial_capital = initial_capital\n",
    "        self.transaction_cost = transaction_cost\n",
    "        \n",
    "        self.assets = price_data.columns.tolist()\n",
    "        self.n_assets = len(self.assets)\n",
    "        \n",
    "        # Observation: predicted returns + current weights\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2*self.n_assets,), dtype=np.float32)\n",
    "        \n",
    "        # Action: change weights (-0.1 to 0.1)\n",
    "        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(self.n_assets,), dtype=np.float32)\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.current_weights = np.ones(self.n_assets)/self.n_assets\n",
    "        self.portfolio_value = self.initial_capital\n",
    "        \n",
    "        # Precompute asset returns\n",
    "        self.asset_returns = self.price_data.pct_change().fillna(0).values\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        obs = np.concatenate([self.predicted_returns[self.current_step], self.current_weights])\n",
    "        return obs.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action to weights\n",
    "        new_weights = self.current_weights + action\n",
    "        new_weights = np.clip(new_weights, 0, None)\n",
    "        if new_weights.sum() == 0:\n",
    "            new_weights = np.ones(self.n_assets)/self.n_assets\n",
    "        else:\n",
    "            new_weights = new_weights / new_weights.sum()\n",
    "        \n",
    "        # Transaction costs\n",
    "        tc_cost = self.transaction_cost * np.sum(np.abs(new_weights - self.current_weights)) * self.portfolio_value\n",
    "        \n",
    "        # Portfolio return\n",
    "        asset_r = self.asset_returns[self.current_step]\n",
    "        port_ret = (self.current_weights @ asset_r)\n",
    "        \n",
    "        self.portfolio_value = self.portfolio_value * (1 + port_ret) - tc_cost\n",
    "        \n",
    "        # Reward: daily return minus TC as a simple metric\n",
    "        reward = port_ret - (tc_cost / self.portfolio_value)\n",
    "        \n",
    "        self.current_weights = new_weights\n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= len(self.asset_returns)-1)\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        info = {'portfolio_value': self.portfolio_value}\n",
    "        return obs, reward, done, False, info\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.current_weights = np.ones(self.n_assets)/self.n_assets\n",
    "        self.portfolio_value = self.initial_capital\n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step}, PV: {self.portfolio_value}, Weights: {self.current_weights}\")\n",
    "\n",
    "\n",
    "# Mock predictions for RL environment (just repeat the BL posterior mean)\n",
    "pred_array = np.tile(mu_post.values, (len(price_data), 1))\n",
    "\n",
    "env = PortfolioEnv(price_data=price_data, predicted_returns=pred_array, transaction_cost=0.001)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=2000)\n",
    "\n",
    "# Test the trained model\n",
    "obs, _ = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Final Portfolio Value after RL simulation:\", info['portfolio_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ed0a8-9db3-44f4-bcb7-dc28557db05d",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "\t•\tBlack-Litterman: Implemented manually with NumPy. No pybl package required.\n",
    "\t•\tTransaction Costs: Incorporated into both BL optimization (static) and RL environment.\n",
    "\t•\tRisk Parity: Provided as an alternative benchmark.\n",
    "\t•\tReinforcement Learning: Demonstrated using a simple PPO agent from stable-baselines3 with gymnasium.\n",
    "\n",
    "This code is an example framework. In practice, you should:\n",
    "\n",
    "\t•\tIntegrate real ML predictions (e.g., from your LSTM model).\n",
    "\t•\tFine-tune RL hyperparameters and improve state and reward functions.\n",
    "\t•\tPerform robust backtesting and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41869a-0efd-47a6-8143-988c99b0c68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyOpt)",
   "language": "python",
   "name": "pyopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
